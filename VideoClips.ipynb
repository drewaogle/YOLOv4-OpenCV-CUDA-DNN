{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef71144-e2c1-4696-a82a-a9c49dae001e",
   "metadata": {},
   "source": [
    "# Creating Video Clips\n",
    "This notebook demonstrates using a YOLOv4 model to process individual frames and use the resulting output to generate information about the video.\n",
    "We use detections of labels over sequential frames to generate Clips which describe the existance of those objects within a specific portion of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835379a-1c02-44d3-930c-f321279ee0bb",
   "metadata": {},
   "source": [
    "# Install ApertureDB\n",
    "First we install the ApertureDB python module and other modules we need for running our model. Then, we verify our connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444a543-c419-4d24-8db4-1fea7ccdc270",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install aperturedb tqdm\n",
    "from aperturedb import Utils\n",
    "c = Utils.create_connector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2a976-8ec0-4e99-a18b-85a1cc454bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = Utils.Utils(c)\n",
    "u.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a96c2-b0d1-47b6-b836-c0eb0142101f",
   "metadata": {},
   "source": [
    "# Download resources\n",
    "Now we need to download the python code to run the model, `yolov4.py` and the video we are going to use, `norman.mp4`, a video about a dog riding a bicycle.\n",
    "\n",
    "This was chosen because it includes several labels but also because it has detections which overlap - dogs and bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053c29f-bbaa-43f3-82e1-bfe103f72579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we retrieve the items we are working with:\n",
    "# Retrieve the YOLO4 interface\n",
    "!rm -f yolo4.py\n",
    "!wget https://raw.githubusercontent.com/drewaogle/YOLOv4-OpenCV-CUDA-DNN/refs/heads/main/yolo4.py\n",
    "# Retreive video\n",
    "!wget https://aperturedata-public.s3.us-west-2.amazonaws.com/aperturedb_applications/norman.mp4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dae152-0643-4701-a505-0b331dd9b204",
   "metadata": {},
   "source": [
    "# Run The Detector\n",
    "Now that we've downloaded our YOLOv4 code, let's run it.\n",
    "This will need to download the weights and some configuration; about 300M and will do it automatically.\n",
    "\n",
    "After downloading or verify files, it will then process the video. with `no_squash_detections` as `True` it won't overwrite an existing output dir, so delete it to rerun. This code can support hardware acceleration, but is designed so it won't be unwieldly without it. Detections should be at about 3-10fps without hardware, and take less 5 minutes.\n",
    "\n",
    "If a file were to fail halfway through it, rerunning the loader wont be happy ( sha2 sums won't match )\n",
    "`!rm ~/models` will reset the downloads, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda14d1-dc17-48da-af51-bdeb8d6e468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import yolo4\n",
    "reload(yolo4)\n",
    "from yolo4 import RemoteYOLOv4\n",
    "class DetectorOptions:\n",
    "    image='' # path for images\n",
    "    stream='' # path for stream\n",
    "    cfg=\"models/yolov4.cfg\" # path to config\n",
    "    weights=\"models/yolov4.weights\" # path to weights\n",
    "    namesfile=\"models/coco.names\" # path for output to name mapping\n",
    "    input_size=416\n",
    "    use_gpu=False # use GPU or not\n",
    "    outdir=\"output/norman\"\n",
    "    no_squash_detections=True # if detections exist, don't rerun.\n",
    "    def __init__(self, image='',stream=''):\n",
    "        self.image = image\n",
    "        self.stream=stream # 'webcam' to open webcam w/ OpenCV\n",
    "\n",
    "# now we pull data\n",
    "dopts = DetectorOptions( stream=\"norman.mp4\")\n",
    "yolo = RemoteYOLOv4.__new__(RemoteYOLOv4)\n",
    "yolo.__init__(dopts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa362d75-8c06-4adb-a0ba-3b66421a1807",
   "metadata": {},
   "source": [
    "## Now Check Detections\n",
    "the YoloV4 code we use outputs detections sequentially into a csv file, so let's load the file and see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3053bdb-a78a-4a8e-a454-2507a1477eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's check detections\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"output/norman/detections.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cd9e9-ecaf-4e67-896b-019a6c30f1c3",
   "metadata": {},
   "source": [
    "# Process Into Clips\n",
    "Now that we've verified that we have the data from the model, we will take the output and process it into Clips.\n",
    "\n",
    "We will define a few classes and functions to process our model output.\n",
    "\n",
    "- `ClipOptions` - an options class that we will use to define how it works;  \n",
    "- `preprocess` - convert the dataframe into information the detection can use\n",
    "- `Clip` - a class that defines our output\n",
    "- `ClipStorage` - a class for maintaining state between the functions\n",
    "- `process_new_frame` - a function to run when we see a new frame\n",
    "- `process_row` - a function to run we we see a new detection\n",
    "- `process` - the function to process the whole video/csv.\n",
    "\n",
    "These could be in a single class, but I've left them apart to allow people to take them piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503ff66-92c7-46a6-9dfb-9b55882c79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Fist we'll define the options we're going to use.\n",
    "class ClipOptions:\n",
    "    offset_frame=0 # starting offset in frames\n",
    "    end_frame=-1 # ending offset in frames\n",
    "    initconf=50 # minimun confidence to start ( 0-100 )\n",
    "    initlen=5 # minimum detection duration in frames to start a clip\n",
    "    dropconf=25 # confidence to end a frame (0 -100 )\n",
    "    droplen=5 # number of detection missed frames to end a clip\n",
    "    detections=None  # path to output detections\n",
    "    video=None # video that the detections is from\n",
    "    verbose=logging.INFO # moderate amount of info\n",
    "    flush=False # remove old uuids\n",
    "    nosave=False # dont add data to db\n",
    "    label=\"\" # label for video\n",
    "    def __init__(self,video,detections):\n",
    "        self.video=video # video file to add\n",
    "        self.detections=detections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9e797-f44f-4b8c-9255-133084dd890e",
   "metadata": {},
   "source": [
    "## Spot Check Detections\n",
    "Lets take a look at the detections when we have defined what the output means, and verify the labeling is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce21ed8-93bb-4d4f-8962-3dc77ce947f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# function to prepare dataframe for work; add columns and trim frames we don't want.\n",
    "def preprocess(df, args ):\n",
    "   processed = df\n",
    "   processed.columns = [\"frame\",\"label\",\"confidence\",\"left\",\"top\",\"width\",\"height\" ]\n",
    "   processed.drop(processed[processed.frame < args.offset_frame].index, inplace=True)\n",
    "   if args.end_frame > -1:\n",
    "      processed.drop(processed[processed.frame > args.end_frame].index,inplace=True)\n",
    "   return processed\n",
    "\n",
    "norman_detects = preprocess( df, opts )\n",
    "print(norman_detects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580724fa-493f-429a-9504-140d412cf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a frame by hand here.\n",
    "\n",
    "from IPython.display import display as ds\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def display_image_and_bb( num, df ):\n",
    "\n",
    "    # we also output the video frames in our model code so we can spot check.\n",
    "    cv_image = cv2.imread( f\"output/norman/video{num}.jpg\")\n",
    "\n",
    "    # Draw a rectangle around the detections\n",
    "    counter = 0\n",
    "    for id,coords in df[df[\"frame\"] == num].iterrows():\n",
    "        left   = coords[\"left\"]\n",
    "        top    = coords[\"top\"]\n",
    "        right  = coords[\"left\"] + coords[\"width\"]\n",
    "        bottom = coords[\"top\"] + coords[\"height\"]\n",
    "        cv2.rectangle(cv_image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(cv_image, coords[\"label\"], (left, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        counter += 1\n",
    "\n",
    "    cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "    ds(Image.fromarray(cv_image_rgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98818068-81a0-4bc9-979e-44fc844ed36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will select a frame, 150 and see now it looks.\n",
    "display_image_and_bb( 150, norman_detects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a56b6-f419-420c-a4ef-7ebff38cb7de",
   "metadata": {},
   "source": [
    "### Detection Verification\n",
    "This is pretty much what we would expect. bike, dog, person .. a car detection in the background, a nice find.\n",
    "\n",
    "That house is *not* a stop sign though. I guess it is seeing the sharp edges and deciding that is the bottom of a stop sign? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6815a-07a8-4fa7-bc42-4f7b2cc89ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Clip class for data storage\n",
    "class Clip:\n",
    "    def __init__(self, label, start,conf):\n",
    "        self.label = label\n",
    "        self.start_frame = start\n",
    "        self.total_frames = 0 # don't include start in total.\n",
    "        self.missed_frames = 0\n",
    "        self.max_confidence = conf\n",
    "        self.min_confidence = conf\n",
    "        self.last_frame_seen = None\n",
    "    def is_active( self, current_frame, drop_len ):\n",
    "        last_seen = self.start_frame + self.total_frames\n",
    "        # drop_len is the number of frame that can be missed and label is \"active\"\n",
    "        # drop_len of 1 means active continues if unseen in previous frame.\n",
    "        return last_seen + drop_len > current_frame\n",
    "    def __str__(self):\n",
    "        return f\"(Clip [{self.label}] @ {self.start_frame} + {self.total_frames})\"\n",
    "    def __repr__(self):\n",
    "        return f\"C[{self.label} @ {self.start_frame} + {self.total_frames}]\"\n",
    "    def describe(self):\n",
    "        return f\"Clip is of {self.label}. First seen at {self.start_frame}, seen until {self.start_frame+self.total_frames}\"\n",
    "    def as_finished(self):\n",
    "        return f\"{self.label}_{self.start_frame+self.total_frames}\"\n",
    "    def add_confidence(self,new_confidence, frame_num):\n",
    "        self.max_confidence = max(self.max_confidence,new_confidence)\n",
    "        self.min_confidence = min(self.min_confidence,new_confidence)\n",
    "        # there could be multiple detections of a given type per frame; we don't track multiple here\n",
    "        # and are merely looking to know how many frames a label occurs in\n",
    "        if frame_num == self.last_frame_seen:\n",
    "            return\n",
    "        self.last_frame_seen = frame_num\n",
    "        self.total_frames = self.total_frames + 1 + self.missed_frames\n",
    "        # when a frame is a hit, we add the missed frames to what is considered the total length.\n",
    "        self.missed_frames = 0 \n",
    "    # frames where confidence was below threshold but kept to avoid drop out.\n",
    "    def add_missed(self,missed_confidence):\n",
    "        self.missed_frames = self.missed_frames + 1\n",
    "\n",
    "class ClipStorage:\n",
    "    def __init__(self):\n",
    "        self.active = {} # clips that have been seen, but not passed initializition count ( suppressed mis-identification )\n",
    "        self.registered = {} # clips that are 'valid', and currently \"seent\"\n",
    "        self.finished = {} # clips that were valid, but dropped off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e8318-bd6f-4520-88dd-f0b801f1a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process events which trigger on new frame.\n",
    "def process_new_frame( verbose, drop_len, cur_frame, last_frame, storage): \n",
    "           # drop any which werent active last frame\n",
    "           new_active = {}\n",
    "           new_registered = {}\n",
    "           for clip in storage.active.values():\n",
    "               if not clip.is_active( cur_frame, drop_len ):\n",
    "                   if verbose >= logging.INFO:\n",
    "                       print(f'New Frame: frame {cur_frame}, Dropped {clip}') \n",
    "               else:   \n",
    "                   if verbose:\n",
    "                       print(f\"New Frame: frame {cur_frame}, kept {clip} in active\")\n",
    "                   new_active[clip.label] = clip\n",
    "           for clip in storage.registered.values():\n",
    "               if not clip.is_active( cur_frame, drop_len ):\n",
    "                   if verbose >= logging.INFO:\n",
    "                       print(f'New Frame: frame {cur_frame}, Retired {clip}') \n",
    "                   storage.finished[ clip.as_finished() ] = clip\n",
    "               else:\n",
    "                   new_registered[clip.label] = clip\n",
    "           if verbose >= logging.DEBUG:\n",
    "               print(f\"Active dict: {storage.active}\")\n",
    "           storage.registered = new_registered\n",
    "           storage.active = new_active   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ae077-3233-485a-b5df-7ffabd6cd382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a row in the detections\n",
    "# YOLOv4 can detect mutitple objects in a frame - this is a single detection in a given frame.\n",
    "def process_row(verbose, initconf, initlen, dropconf, cur_frame, label, label_confidence, storage):\n",
    "       if label in storage.active.keys():\n",
    "           clip = storage.active[label]\n",
    "           if label_confidence * 100 > initconf:\n",
    "               clip.add_confidence(label_confidence,cur_frame)\n",
    "               # total frames doesn't include first frame, so add 1.\n",
    "               if clip.total_frames +1 >= initlen: \n",
    "                   if verbose >= logging.INFO:\n",
    "                       print(f\"At {cur_frame}, moved {clip} to registered\")\n",
    "                   storage.registered[label] = clip\n",
    "                   del storage.active[label]\n",
    "               else:\n",
    "                   if verbose >= logging.INFO:\n",
    "                       print(f\"At frame {cur_frame}, saw {clip}\")\n",
    "           else:   \n",
    "               if verbose >= logging.INFO:\n",
    "                   print(f\"{clip} seen at frame {cur_frame}, but confidence [ {label_confidence*100} < { initconf }]\" )\n",
    "       elif label in storage.registered.keys():\n",
    "           clip = storage.registered[label]\n",
    "           # if above confidence for dropping, consider a new registered frame\n",
    "           if label_confidence * 100 > dropconf:\n",
    "                # allows frame to miss one and restart; duration calculated from start to current.\n",
    "                clip.add_confidence(label_confidence,cur_frame)\n",
    "           else:\n",
    "               clip.add_missed(label_confidence)\n",
    "       else:    \n",
    "           # if label not in active list, nor registered.\n",
    "           if label_confidence * 100 > initconf:\n",
    "               clip = Clip( label, cur_frame, label_confidence )\n",
    "               if verbose >= logging.INFO:\n",
    "                   print(f\"* Added {clip} to actived\")\n",
    "               storage.active[label] = clip\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b192263-c3d8-4de3-87d2-d3ec26e3ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop over a frame.\n",
    "def process(args,pf):\n",
    "    args.verbose = True\n",
    "    clip_store = ClipStorage()\n",
    "    last_frame =0\n",
    "    cur_frame = 0\n",
    "    for idx,row in pf.iterrows():\n",
    "        cur_frame = row['frame']\n",
    "        label = row['label']\n",
    "        if cur_frame > 155:\n",
    "            break\n",
    "        if cur_frame != last_frame:\n",
    "           if args.verbose >= logging.DEBUG:\n",
    "               print(f\"Processing switch from {last_frame} to {cur_frame}\")\n",
    "           process_new_frame(args.verbose,args.droplen, cur_frame, last_frame,clip_store)\n",
    "\n",
    "        # all old active and registered are dropped prior to this.\n",
    "        process_row(args.verbose, args.initconf, args.initlen, args.dropconf,cur_frame,label,row['confidence'],clip_store)\n",
    "\n",
    "        last_frame = cur_frame\n",
    "    # move all registered to finished.\n",
    "    if args.verbose:\n",
    "        print(\"Video complete, finishing clips\")\n",
    "        for clip in clip_store.registered.values():\n",
    "            clip_store.finished[ clip.as_finished() ] = clip\n",
    "    return clip_store.finished\n",
    "       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a06c6-2d8a-471d-b1f5-de72bb4c7af0",
   "metadata": {},
   "source": [
    "## Run the Clip Processing\n",
    "Now that we've defined all our functions, we'll run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f241-edad-4817-b07f-75de6565960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "opts = ClipOptions( \"norman.mp4\",\"output/noman/detections.csv\" )\n",
    "opts.label=\"Norman_Bike\"\n",
    "opts.initconf=45\n",
    "opts.initlen=3\n",
    "opts.dropconf=20\n",
    "opts.droplen=3\n",
    "\n",
    "norman_finished = process(opts,norman_detects)\n",
    "#print(norman_finished)\n",
    "for clip in norman_finished.values():\n",
    "    print(clip.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed405d-1f6f-4a69-af95-85d3b7bf790a",
   "metadata": {},
   "source": [
    "# Adding the Results to ApertureDB\n",
    "Now we have some data that we can put into the database.\n",
    "We'll make some functions to handle the different types of data we're adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac7dc5-88bd-48d7-8795-1635243bd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Detections to Database\n",
    "import uuid\n",
    "\n",
    "video_url = \"aperturedb://demos/video_clip/video/{0}\"\n",
    "frame_url = \"video_clips://frame/{0}\"\n",
    "clip_url = \"video_clips://clip/{0}\"\n",
    "\n",
    "u.remove_all_objects()\n",
    "\n",
    "def run_query(db, query,blobs,action_desc):\n",
    "    blobs = [] if blobs is None else blobs\n",
    "    #print(query)\n",
    "    result,_ = db.query(query,blobs)\n",
    "    if not db.last_query_ok():\n",
    "        raise Exception(f\"Failed Running Query for {action_desc}: {result}\")\n",
    "    return result\n",
    "\n",
    "def add_detections( db, video_id, detections ):\n",
    "    det_df = pd.read_csv( detections )\n",
    "    det_df.columns = [\"frame\",\"label\",\"confidence\",\"left\",\"top\",\"width\",\"height\" ]\n",
    "    def format_detections( detections ):\n",
    "        return \"\".join( f\"[{det.frame},{det.label},{det.confidence},{det.left},{det.top},{det.width},{det.height}]\" for det in detections )\n",
    "\n",
    "    frame_number = 0\n",
    "    frame_detections = []\n",
    "    def on_end_frame( detections ):\n",
    "        add_frame_query=[{\n",
    "            \"FindVideo\": {\n",
    "                \"constraints\": {\n",
    "                    \"id\": [\"==\",str(video_id)]\n",
    "                },\n",
    "                \"_ref\":1\n",
    "            }\n",
    "        },{\n",
    "            \"AddFrame\": {\n",
    "                \"video_ref\":1,\n",
    "                \"frame_number\": frame_number,\n",
    "                \"properties\": {\n",
    "                    \"detections\": format_detections( detections ),\n",
    "                     \"frame_number\":frame_number,\n",
    "                      \"id\": str(frame_id)\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        run_query(db,add_frame_query,None, \"Adding Frame\")\n",
    "        add_bboxes(db,frame_id,detections)\n",
    "\n",
    "    for row,data in det_df.iterrows():\n",
    "        if data['frame'] != frame_number:\n",
    "            on_end_frame(frame_detections)\n",
    "            frame_detections = []\n",
    "        frame_number = data['frame']\n",
    "        frame_detections.append( data )\n",
    "    # output last frame\n",
    "    on_end_frame(frame_detections)\n",
    "        \n",
    "\n",
    "def add_clips(db,video_id,clips):\n",
    "    for clip in clips:\n",
    "        add_clip_query=[{\n",
    "            \"FindVideo\": {\n",
    "                \"constraints\": {\n",
    "                    \"id\": [\"==\",str(video_id)]\n",
    "                },\n",
    "                \"_ref\":1\n",
    "            }\n",
    "        },{\n",
    "            \"AddClip\": {\n",
    "                \"video_ref\":1,\n",
    "                \"frame_number_range\":{\n",
    "                    \"start\": clip.start_frame,\n",
    "                    \"stop\": clip.start_frame + clip.total_frames\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"label\": clip.label,\n",
    "                    \"id\": str(uuid.uuid5( video_id, clip_url.format( f\"{clip.label}_{clip.start_frame}\" )))\n",
    "                },\n",
    "            }\n",
    "        }]\n",
    "        print(run_query(db,add_clip_query,None, \"Add Clip\"))\n",
    "        print(add_clip_query)\n",
    "        acq2=[add_clip_query[0]]\n",
    "        acq2[0][\"FindVideo\"][\"results\"] = {\"count\":True }\n",
    "        print(run_query(db,acq2,None, \"AC Test\"))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def add_video( db, video_path, detections_path, video_description ):\n",
    "    video_id = uuid.uuid5( uuid.NAMESPACE_URL, video_url.format( video_path ))\n",
    "    add_video_query=[{\n",
    "        \"AddVideo\": {\n",
    "            \"properties\": {\n",
    "                \"source\": video_path,\n",
    "                \"descrption\": video_description,\n",
    "                \"id\": str( video_id )\n",
    "            }\n",
    "        }\n",
    "        }]\n",
    "    fd = open( video_path, 'rb')\n",
    "    video_data = fd.read()\n",
    "    fd.close()\n",
    "\n",
    "    info_video_query=[{\n",
    "        \"FindVideo\":{\n",
    "            \"constraints\": {\n",
    "                \"id\": [\"==\",str(video_id)]\n",
    "            },\n",
    "            \"results\": {\n",
    "                \"all_properties\":True\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    \n",
    "    \n",
    "    run_query(db,add_video_query,[video_data],\"Video Adding\")\n",
    "    print(run_query(db,info_video_query,None,\"Video Find\"))\n",
    "    \n",
    "    add_detections( db, video_id, detections_path )\n",
    "    add_clips( db, video_id,  norman_finished.values() )\n",
    "try:\n",
    "    add_video( c, \"norman.mp4\", \"output/norman/detections.csv\" , \"Norman the dog rides a bike with some help\")\n",
    "    u.summary()\n",
    "except Exception as e:\n",
    "    print(\"Failed adding: \",e)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4e5f6-f8ef-451f-905a-26b33072c292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40efdaea-a2a8-4d36-85dc-c8d74bcead4a",
   "metadata": {},
   "source": [
    "# Clip Verification\n",
    "It looks like we found a lot of the things we wanted to find, a dog, a bicycle, and the lady who was helping the dorg.\n",
    "\n",
    "It's odd that we don't see the dog until frame 136 though, what gives?\n",
    "\n",
    "Also, our output shows it first registering the dog at frame 120, but then losing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d2a10-557b-4fb2-9f37-a5fa3ca7a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's odd that the dog is not seen until clip 136, lets see why?\n",
    "\n",
    "## at this point we will assume you've figured this out after "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
